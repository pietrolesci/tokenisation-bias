# https://huggingface.co/HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints/blob/step-1200000/config.json
name: me100M-tied
model_type: llama
hidden_act: silu
hidden_size: 576
intermediate_size: 1536
num_attention_heads: 9
num_key_value_heads: 3
num_hidden_layers: 30
tie_word_embeddings: true
initializer_range: 0.041666666666666664
attention_bias: false
attention_dropout: 0.0
mlp_bias: false
pretraining_tp: 1
rms_norm_eps: 1e-05
rope_scaling: null
rope_theta: 10000.0
rope_interleaved: false