# https://huggingface.co/HuggingFaceTB/SmolLM-360M/blob/main/config.json
name: me340M-tied
model_type: llama
hidden_act: silu
hidden_size: 1024
intermediate_size: 2560
num_attention_heads: 15
num_key_value_heads: 5
num_hidden_layers: 32
tie_word_embeddings: true
initializer_range: 0.02
attention_bias: false
attention_dropout: 0.0
mlp_bias: false
pretraining_tp: 1
rms_norm_eps: 1e-05
rope_scaling: null
rope_theta: 10000.0