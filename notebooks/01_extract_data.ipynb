{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import srsly\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from commands.utilities import MODEL_EVAL_PATH, TOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/me-minipile-evals/me100M_finewebedu-20B_bpe32000minipile.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='100M', trainset='finewebedu-20B', tokenizer='bpe32000minipile', vocab_size=32000, window_size=5000\n"
     ]
    }
   ],
   "source": [
    "window_size = 5000\n",
    "\n",
    "run_name = Path(path).stem\n",
    "s = run_name.split(\"_\")\n",
    "model = s[0].removeprefix(\"me\")\n",
    "trainset = s[1]\n",
    "tokenizer = s[2]\n",
    "vocab_size = int(max(re.findall(\"\\d+\", tokenizer), key=len))\n",
    "\n",
    "print(f\"{model=}, {trainset=}, {tokenizer=}, {vocab_size=}, {window_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_path = Path(f\"{TOK_PATH}/{tokenizer}\")\n",
    "data_path = Path(f\"data/minipile/{tokenizer}/test\")\n",
    "eval_path = Path(f\"{MODEL_EVAL_PATH}/{run_name}.parquet\")\n",
    "\n",
    "assert tok_path.exists(), f\"Tokenizer path {tok_path} does not exist.\"\n",
    "assert data_path.exists(), f\"Data path {data_path} does not exist.\"\n",
    "assert eval_path.exists(), f\"Eval file {eval_path} does not exist.\"\n",
    "\n",
    "out_path = Path(\"data/me-minipile-regdata\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263 tokens dropped because are part of other tokens in the window (window size: 5000 * 2)\n"
     ]
    }
   ],
   "source": [
    "with (tok_path / \"raw_tok_path.txt\").open(\"r\") as fl:\n",
    "    raw_tok_path = Path(fl.read())\n",
    "\n",
    "merges_df = (\n",
    "    pl.DataFrame(srsly.read_jsonl(raw_tok_path / \"implemented_merges.jsonl\"))\n",
    "    .with_columns(tok_a=pl.col(\"pair\").list.get(0), tok_b=pl.col(\"pair\").list.get(1))\n",
    "    .drop([\"pair\", \"new_token\", \"part_a\", \"part_b\"])\n",
    "    .rename({\"new_token_id\": \"tok\"})\n",
    ")\n",
    "\n",
    "# Filter merges based on the window size and vocab size\n",
    "merges_df = (\n",
    "    merges_df.filter((pl.col(\"tok\") < vocab_size + window_size) & (pl.col(\"tok\") >= vocab_size - window_size))\n",
    "    .sort(\"tok\")\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "# Find tokens (in-vocab) that got merged into others, either as first or second part of the token\n",
    "to_drop = pl.concat(\n",
    "    [\n",
    "        merges_df.filter(pl.col(\"tok\") < vocab_size).join(\n",
    "            merges_df.select([\"tok\", col]), left_on=\"tok\", right_on=col, how=\"inner\", suffix=\"_new\"\n",
    "        )\n",
    "        for col in [\"tok_a\", \"tok_b\"]\n",
    "    ]\n",
    ").select([\"tok\", \"tok_new\"])\n",
    "print(f\"{len(to_drop)} tokens dropped because are part of other tokens in the window (window size: {window_size} * 2)\")\n",
    "merges_df = merges_df.filter(pl.col(\"tok\").is_in(to_drop[\"tok\"].implode()).not_())\n",
    "\n",
    "# We only need this to get the tokens composing the OOV tokens\n",
    "merges_df = merges_df.filter(pl.col(\"tok\") >= vocab_size)  # notice the '='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and align evaluations and dataset\n",
    "\n",
    "From this section we get back the original dataset where documents are aligned with evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of data\n",
      " shape: (5, 2)\n",
      "┌──────────────────────┬─────────┐\n",
      "│ input_ids            ┆ uid     │\n",
      "│ ---                  ┆ ---     │\n",
      "│ list[u16]            ┆ i64     │\n",
      "╞══════════════════════╪═════════╡\n",
      "│ [899, 1390, … 14]    ┆ 1000500 │\n",
      "│ [19181, 3048, … 199] ┆ 1000501 │\n",
      "│ [5318, 29501, … 14]  ┆ 1000502 │\n",
      "│ [27064, 284, … 14]   ┆ 1000503 │\n",
      "│ [2141, 11758, … 14]  ┆ 1000504 │\n",
      "└──────────────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pl.from_arrow(load_from_disk(data_path).data.table).shrink_to_fit()\n",
    "doc_len = data.with_columns(len=pl.col(\"input_ids\").list.len().cast(pl.Int64)).drop(\"input_ids\")\n",
    "\n",
    "print(\"Example of data\\n\", data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of evaluations\n",
      " shape: (5, 4)\n",
      "┌─────────┬─────────────────────────────────┬─────────────────────┬──────┐\n",
      "│ uid     ┆ token_logprob                   ┆ token_ids           ┆ step │\n",
      "│ ---     ┆ ---                             ┆ ---                 ┆ ---  │\n",
      "│ i64     ┆ list[f64]                       ┆ list[i64]           ┆ i64  │\n",
      "╞═════════╪═════════════════════════════════╪═════════════════════╪══════╡\n",
      "│ 1009373 ┆ [-10.568663, -12.305836, … -10… ┆ [199, 9389, … 199]  ┆ 0    │\n",
      "│ 1000646 ┆ [-10.00154, -10.200759, … -10.… ┆ [199, 3, … 199]     ┆ 0    │\n",
      "│ 1004697 ┆ [-10.943292, -10.172346, … -10… ┆ [199, 14672, … 199] ┆ 0    │\n",
      "│ 1000604 ┆ [-10.00154, -10.669509, … -12.… ┆ [199, 4144, … 688]  ┆ 0    │\n",
      "│ 1003502 ┆ [-10.655336, -10.646159, … -11… ┆ [199, 1507, … 221]  ┆ 0    │\n",
      "└─────────┴─────────────────────────────────┴─────────────────────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "# Read (a subset) of the evaluations and add step column\n",
    "df = (\n",
    "    pl.scan_parquet(eval_path)\n",
    "    .filter(pl.col(\"step\") == 0)\n",
    "    # pl.scan_parquet(list(path.rglob(\"step0*.parquet\")), include_file_paths=\"filename\")\n",
    "    # .with_columns(step=pl.col(\"filename\").str.extract(\"step(\\d+)\").cast(pl.Int64))\n",
    "    # .drop(\"filename\")\n",
    "    .collect()\n",
    "    .shrink_to_fit()\n",
    ")\n",
    "\n",
    "print(\"Example of evaluations\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of evaluations with computed offset\n",
      " shape: (5, 7)\n",
      "┌─────────┬──────────────────────┬─────────────────────┬──────┬────────┬────────┬──────────────────┐\n",
      "│ uid     ┆ token_logprob        ┆ token_ids           ┆ step ┆ len    ┆ offset ┆ pos              │\n",
      "│ ---     ┆ ---                  ┆ ---                 ┆ ---  ┆ ---    ┆ ---    ┆ ---              │\n",
      "│ i64     ┆ list[f64]            ┆ list[i64]           ┆ i64  ┆ i64    ┆ i64    ┆ list[i64]        │\n",
      "╞═════════╪══════════════════════╪═════════════════════╪══════╪════════╪════════╪══════════════════╡\n",
      "│ 1009373 ┆ [-10.568663,         ┆ [199, 9389, … 199]  ┆ 0    ┆ 141790 ┆ 141789 ┆ [0, 1, … 141788] │\n",
      "│         ┆ -12.305836, … -10…   ┆                     ┆      ┆        ┆        ┆                  │\n",
      "│ 1000646 ┆ [-10.00154,          ┆ [199, 3, … 199]     ┆ 0    ┆ 136885 ┆ 136884 ┆ [0, 1, … 136883] │\n",
      "│         ┆ -10.200759, … -10.…  ┆                     ┆      ┆        ┆        ┆                  │\n",
      "│ 1004697 ┆ [-10.943292,         ┆ [199, 14672, … 199] ┆ 0    ┆ 129162 ┆ 129161 ┆ [0, 1, … 129160] │\n",
      "│         ┆ -10.172346, … -10…   ┆                     ┆      ┆        ┆        ┆                  │\n",
      "│ 1000604 ┆ [-10.00154,          ┆ [199, 4144, … 688]  ┆ 0    ┆ 121282 ┆ 121281 ┆ [0, 1, … 121280] │\n",
      "│         ┆ -10.669509, … -12.…  ┆                     ┆      ┆        ┆        ┆                  │\n",
      "│ 1003502 ┆ [-10.655336,         ┆ [199, 1507, … 221]  ┆ 0    ┆ 110204 ┆ 110203 ┆ [0, 1, … 110202] │\n",
      "│         ┆ -10.646159, … -11…   ┆                     ┆      ┆        ┆        ┆                  │\n",
      "└─────────┴──────────────────────┴─────────────────────┴──────┴────────┴────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df.join(doc_len, on=\"uid\", how=\"left\")\n",
    "    # consider that minimum between the original length of the doc and the evaluation\n",
    "    # there might be differences becausa when we have padding then the first token is in\n",
    "    # the evaluations; when there is no padding, the first token is only used as context\n",
    "    .with_columns(offset=pl.min_horizontal(pl.col(\"token_logprob\").list.len(), pl.col(\"len\")))\n",
    "    .with_columns(\n",
    "        pl.col(\"token_logprob\").list.slice(-pl.col(\"offset\")), pl.col(\"token_ids\").list.slice(-pl.col(\"offset\"))\n",
    "    )\n",
    "    .with_columns(pos=pl.int_ranges(0, pl.col(\"token_logprob\").list.len()))\n",
    ")\n",
    "doc_len_check = df.select([\"uid\", \"offset\"])\n",
    "\n",
    "print(\"Example of evaluations with computed offset\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of aligned dataset\n",
      " shape: (5, 2)\n",
      "┌─────────────────────┬─────────┐\n",
      "│ input_ids           ┆ uid     │\n",
      "│ ---                 ┆ ---     │\n",
      "│ list[u16]           ┆ i64     │\n",
      "╞═════════════════════╪═════════╡\n",
      "│ [899, 1390, … 14]   ┆ 1000500 │\n",
      "│ [3048, 3660, … 199] ┆ 1000501 │\n",
      "│ [5318, 29501, … 14] ┆ 1000502 │\n",
      "│ [27064, 284, … 14]  ┆ 1000503 │\n",
      "│ [2141, 11758, … 14] ┆ 1000504 │\n",
      "└─────────────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "data = (\n",
    "    data.join(doc_len_check, on=\"uid\", how=\"left\")\n",
    "    # since offset might be shorted than len doc, we need to slice the original\n",
    "    # data as well\n",
    "    .with_columns(pl.col(\"input_ids\").list.slice(-pl.col(\"offset\")))\n",
    "    .drop(\"offset\")\n",
    ")\n",
    "\n",
    "print(\"Example of aligned dataset\\n\", data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align un-merged tokens with the merges\n",
    "\n",
    "For two rows of unmerged tokens, say 1 and 2, we will add another column where the values will be the token_id of the merged token that contains the two tokens. If the two tokens are not merged, the value will be the token_id of the token itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    data\n",
    "    # Create position: since we aligned it in the previous section, this now works!\n",
    "    .with_columns(pos=pl.int_ranges(0, pl.col(\"input_ids\").list.len()))\n",
    "    .explode([\"input_ids\", \"pos\"])\n",
    "    # Rename for convenience\n",
    "    .rename({\"input_ids\": \"tok\"})\n",
    "    # Change data type for later\n",
    "    .with_columns(pl.col(\"tok\").cast(pl.Int64))\n",
    ")\n",
    "\n",
    "# ==== Find OOV tokens ====\n",
    "# (i) Match the first token of the pair\n",
    "data = data.with_columns(next_tok=pl.col(\"tok\").shift(-1).over(\"uid\")).join(\n",
    "    merges_df, left_on=[\"tok\", \"next_tok\"], right_on=[\"tok_a\", \"tok_b\"], how=\"left\", suffix=\"_oov_a\", nulls_equal=False\n",
    ")\n",
    "\n",
    "# (ii) Match the second token of the pair and keep the\n",
    "# position of the first token of the pair\n",
    "# in this way we tag both rows of pair_a and pair_b with new_tok label\n",
    "data = data.join(\n",
    "    (data.filter(pl.col(\"tok_oov_a\").is_not_null()).with_columns(next_pos=pl.col(\"pos\") + 1).drop([\"tok\"])),\n",
    "    left_on=[\"uid\", \"tok\", \"pos\"],\n",
    "    right_on=[\"uid\", \"next_tok\", \"next_pos\"],\n",
    "    how=\"left\",\n",
    "    suffix=\"_b\",\n",
    "    nulls_equal=False,\n",
    ")\n",
    "\n",
    "# # Check it works\n",
    "# data.filter((pl.col(\"uid\") == 1000500) & (pl.col(\"pos\") > 60) & (pl.col(\"pos\") < 70))\n",
    "# merges_df.filter(pl.col(\"tok\") == 8434)\n",
    "\n",
    "# Clean up\n",
    "data = (\n",
    "    data.rename({\"tok\": \"og_tok\", \"pos\": \"og_pos\"})\n",
    "    .with_columns(tok=pl.coalesce([\"tok_oov_a\", \"tok_oov_a_b\", \"og_tok\"]), pos=pl.min_horizontal([\"pos_b\", \"og_pos\"]))\n",
    "    .drop([\"next_tok\", \"tok_oov_a\", \"tok_oov_a_b\", \"pos_b\"])\n",
    "    .sort([\"uid\", \"pos\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of dataset aligned with merges. 'og' columns are the original columns\n",
      " shape: (5, 5)\n",
      "┌────────┬─────────┬────────┬───────┬─────┐\n",
      "│ og_tok ┆ uid     ┆ og_pos ┆ tok   ┆ pos │\n",
      "│ ---    ┆ ---     ┆ ---    ┆ ---   ┆ --- │\n",
      "│ i64    ┆ i64     ┆ i64    ┆ i64   ┆ i64 │\n",
      "╞════════╪═════════╪════════╪═══════╪═════╡\n",
      "│ 899    ┆ 1000500 ┆ 0      ┆ 899   ┆ 0   │\n",
      "│ 1390   ┆ 1000500 ┆ 1      ┆ 1390  ┆ 1   │\n",
      "│ 298    ┆ 1000500 ┆ 2      ┆ 298   ┆ 2   │\n",
      "│ 1195   ┆ 1000500 ┆ 3      ┆ 1195  ┆ 3   │\n",
      "│ 20798  ┆ 1000500 ┆ 4      ┆ 20798 ┆ 4   │\n",
      "└────────┴─────────┴────────┴───────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of dataset aligned with merges. 'og' columns are the original columns\\n\", data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the evaluations\n",
    "\n",
    "There is a bit of redundant code here, but it's not too bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ALL evaluations and add step column\n",
    "df = (\n",
    "    pl.scan_parquet(eval_path)\n",
    "    # Apply the same offset as before\n",
    "    .join(doc_len.lazy(), on=\"uid\", how=\"left\")\n",
    "    .with_columns(offset=pl.min_horizontal(pl.col(\"token_logprob\").list.len(), pl.col(\"len\")))\n",
    "    .with_columns(\n",
    "        pl.col(\"token_logprob\").list.slice(-pl.col(\"offset\")), pl.col(\"token_ids\").list.slice(-pl.col(\"offset\"))\n",
    "    )\n",
    "    .with_columns(pos=pl.int_ranges(0, pl.col(\"token_logprob\").list.len()))\n",
    "    .drop([\"offset\", \"len\"])\n",
    ")\n",
    "\n",
    "# print(\"Example of evaluations with computed offset\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick check that the data and evaluations are aligned\n",
    "# a = df.head(100).collect()\n",
    "# a = a.explode([\"token_logprob\", \"token_ids\", \"pos\"])\n",
    "# c = a.join(data, left_on=[\"uid\", \"token_ids\", \"pos\"], right_on=[\"uid\", \"og_tok\", \"og_pos\"], how=\"inner\")\n",
    "# assert len(a) == len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the tokens around the cutoff\n",
    "subset_data = data.filter((pl.col(\"tok\") >= vocab_size - window_size) & (pl.col(\"tok\") < vocab_size + window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = (\n",
    "    df.explode([\"token_logprob\", \"token_ids\", \"pos\"])\n",
    "    .join(subset_data.lazy(), left_on=[\"uid\", \"token_ids\", \"pos\"], right_on=[\"uid\", \"og_tok\", \"og_pos\"], how=\"inner\")\n",
    "    .collect()\n",
    ")\n",
    "# check that we match all tokens in subset_data\n",
    "assert eval_df.group_by(\"step\").len()[\"len\"].unique().item() == len(subset_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for regression\n",
    "\n",
    "We sum the log-likelihood of OOV-tokens to that we get their probability as if they are one. Then, we aggregate across documents (i.e., average across contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = (\n",
    "    eval_df\n",
    "    # here pos_right allows us to not group for OOV tokens at different positions in the same doc\n",
    "    .group_by([\"step\", \"uid\", \"tok\", \"pos_right\"])\n",
    "    .agg(pl.col(\"token_logprob\").sum())\n",
    "    # average across contexts\n",
    "    .group_by([\"step\", \"tok\"])\n",
    "    .agg(\n",
    "        median=pl.col(\"token_logprob\").quantile(0.5),\n",
    "        q25=pl.col(\"token_logprob\").quantile(0.25),\n",
    "        q75=pl.col(\"token_logprob\").quantile(0.75),\n",
    "        mean=pl.col(\"token_logprob\").mean(),\n",
    "        std=pl.col(\"token_logprob\").std(),\n",
    "        num=pl.len(),\n",
    "    )\n",
    "    .with_columns(iqr=pl.col(\"q75\") - pl.col(\"q25\"), treat=pl.col(\"tok\") < vocab_size)\n",
    "    .drop([\"q25\", \"q75\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.write_parquet(out_path / f\"{run_name}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
