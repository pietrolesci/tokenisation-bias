{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import srsly\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get dataset\n",
    "# splits = {'validation': 'data/validation-00000-of-00001-a2192e61a091cecb.parquet', 'test': 'data/test-00000-of-00001-010a6231c4b54d31.parquet'}\n",
    "# df = pl.concat([\n",
    "#     pl.scan_parquet(f'hf://datasets/JeanKaddour/minipile/{v}').with_columns(split=pl.lit(k)) \n",
    "#     for k, v in splits.items()\n",
    "# ]).collect()\n",
    "# df = df.with_row_index().rename({\"index\": \"uid\"})\n",
    "# df.write_parquet(\"data/minipile-eval.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_type = \"bpe\"\n",
    "vocab_size = 32_000\n",
    "dataset = \"minipile\"\n",
    "tok_path = Path(f\"./outputs/tokenizers/{tok_type}{vocab_size}{dataset}\")\n",
    "out_path = Path(f\"data/{dataset}-eval-{tok_type}{vocab_size}{dataset}\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenized corpus\n",
    "# file_path = out_path / \"data.parquet\"\n",
    "# if file_path.exists():\n",
    "#     df = pl.read_parquet(file_path)\n",
    "# else:\n",
    "#     df = (\n",
    "#         pl.scan_parquet(f\"hf://datasets/pietrolesci/slim-pajama-eval/{tok_type}{vocab_size}/train-*.parquet\")\n",
    "#         .with_columns(tok_pos=pl.int_ranges(pl.col(\"input_ids\").list.len()), seq_len=pl.col(\"input_ids\").list.len())\n",
    "#         .collect()\n",
    "#     )\n",
    "#     df.write_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(\"data/minipile-eval-bpe32000minipile.parquet\")\n",
    "    .with_columns(tok_pos=pl.int_ranges(pl.col(\"input_ids\").list.len()), seq_len=pl.col(\"input_ids\").list.len())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merges\n",
    "with (tok_path / \"raw_tok_path.txt\").open(\"r\") as fl:\n",
    "    raw_tok_path = Path(fl.read())\n",
    "\n",
    "merges_df = pl.DataFrame(srsly.read_jsonl(raw_tok_path / \"implemented_merges.jsonl\")).with_columns(\n",
    "    pl.col(\"new_token_id\").cast(pl.Int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get window around cutoff\n",
    "num_tok_window = 1_500\n",
    "window_df = merges_df.filter(\n",
    "    (pl.col(\"new_token_id\") >= vocab_size - num_tok_window) & (pl.col(\"new_token_id\") < vocab_size + num_tok_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for in- and out- of vocab tokens around the cutoff\n",
    "in_vocab = window_df.filter(pl.col(\"new_token_id\") < vocab_size)\n",
    "\n",
    "out_vocab = window_df.filter(pl.col(\"new_token_id\") >= vocab_size)\n",
    "out_vocab = (\n",
    "    out_vocab.with_columns(pl.col(\"pair\").list.to_struct())\n",
    "    .unnest(\"pair\")\n",
    "    .rename({\"field_0\": \"tok_a\", \"field_1\": \"tok_b\"})\n",
    "    .with_columns(pl.col(\"tok_a\").cast(pl.Int32), pl.col(\"tok_b\").cast(pl.Int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want at least 3 tokens in the context because\n",
    "# we evaluate on 2 tokens and need 1 for context\n",
    "dfe = df.lazy().explode([\"input_ids\", \"tok_pos\"]).filter(pl.col(\"tok_pos\") >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get uid and tok_pos for tokens in window\n",
    "in_vocab_index = (\n",
    "    in_vocab.lazy().select([\"new_token_id\"]).join(dfe, left_on=\"new_token_id\", right_on=\"input_ids\", how=\"inner\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index = (\n",
    "    out_vocab.lazy()\n",
    "    .select([\"tok_a\", \"tok_b\", \"new_token_id\"])\n",
    "    .join(\n",
    "        dfe.with_columns(next_input_id=pl.col(\"input_ids\").shift(-1)),\n",
    "        left_on=[\"tok_a\", \"tok_b\"],\n",
    "        right_on=[\"input_ids\", \"next_input_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .rename({\"tok_pos\": \"tok_pos_a\"})\n",
    "    .with_columns(tok_pos_b=pl.col(\"tok_pos_a\") + 1)\n",
    "    .filter(pl.col(\"tok_pos_b\") < pl.col(\"seq_len\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check\n",
    "# window_df.filter(\n",
    "#     pl.col(\"new_token_id\")\n",
    "#     .is_in(out_vocab_index[\"new_token_id\"].to_list() + in_vocab_index[\"new_token_id\"].to_list())\n",
    "#     .not_()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample num_samples docs for each token\n",
    "num_samples = 100\n",
    "\n",
    "in_vocab_sample = in_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ").collect()\n",
    "\n",
    "out_vocab_sample = out_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context\n",
    "context_length = 2048\n",
    "\n",
    "in_vocab_df = (\n",
    "    in_vocab_sample.join(df.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos\") > context_length).then(pl.col(\"tok_pos\") - context_length).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos\") - pl.col(\"context_start\") + 1\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# check that last token in context is exactly the token we want to predict\n",
    "assert in_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"new_token_id\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df = (\n",
    "    out_vocab_sample.join(df.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos_b\") > context_length).then(pl.col(\"tok_pos_a\") - context_length + 1).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos_a\") - pl.col(\"context_start\") + 2\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# Check that penultimate token in context is exactly the first token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-2) == pl.col(\"tok_a\"))[\"context\"].all()\n",
    "\n",
    "# Check that last token in context is exactly the second token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"tok_b\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df = in_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\"new_token_id\", \"uid\", \"seq_len\", \"tok_pos\", \"context_start\", \"context_len\", \"context\"]\n",
    ")\n",
    "\n",
    "out_vocab_df = out_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\n",
    "        \"new_token_id\",\n",
    "        \"tok_a\",\n",
    "        \"tok_b\",\n",
    "        \"uid\",\n",
    "        \"seq_len\",\n",
    "        \"tok_pos_a\",\n",
    "        \"tok_pos_b\",\n",
    "        \"context_start\",\n",
    "        \"context_len\",\n",
    "        \"context\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df.write_parquet(out_path / \"in_vocab_samples.parquet\")\n",
    "out_vocab_df.write_parquet(out_path / \"out_vocab_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df = pl.read_parquet(out_path / \"in_vocab_samples.parquet\")\n",
    "out_vocab_df = pl.read_parquet(out_path / \"out_vocab_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040f5d9bc94d464bac68a36d1fe4e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/100708 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_context = pl.concat(\n",
    "    [\n",
    "        in_vocab_df.rename({\"context\": \"input_ids\"}).select([\"new_token_id\", \"uid\", \"input_ids\"]),\n",
    "        out_vocab_df.rename({\"context\": \"input_ids\"}).select([\"new_token_id\", \"uid\", \"input_ids\"]),\n",
    "    ]\n",
    ")\n",
    "all_context = all_context.sort(pl.col(\"input_ids\").list.len(), descending=True)\n",
    "ds = Dataset.from_polars(all_context)\n",
    "ds.save_to_disk(out_path / \"eval_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_703, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>new_token_id</th><th>count</th></tr><tr><td>i32</td><td>u64</td></tr></thead><tbody><tr><td>31822</td><td>54</td></tr><tr><td>33238</td><td>20</td></tr><tr><td>31975</td><td>20</td></tr><tr><td>32997</td><td>22</td></tr><tr><td>31161</td><td>39</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>33022</td><td>2</td></tr><tr><td>33420</td><td>6</td></tr><tr><td>33300</td><td>23</td></tr><tr><td>32742</td><td>16</td></tr><tr><td>32792</td><td>12</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_703, 2)\n",
       "┌──────────────┬───────┐\n",
       "│ new_token_id ┆ count │\n",
       "│ ---          ┆ ---   │\n",
       "│ i32          ┆ u64   │\n",
       "╞══════════════╪═══════╡\n",
       "│ 31822        ┆ 54    │\n",
       "│ 33238        ┆ 20    │\n",
       "│ 31975        ┆ 20    │\n",
       "│ 32997        ┆ 22    │\n",
       "│ 31161        ┆ 39    │\n",
       "│ …            ┆ …     │\n",
       "│ 33022        ┆ 2     │\n",
       "│ 33420        ┆ 6     │\n",
       "│ 33300        ┆ 23    │\n",
       "│ 32742        ┆ 16    │\n",
       "│ 32792        ┆ 12    │\n",
       "└──────────────┴───────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s# Check\n",
    "all_context[\"new_token_id\"].value_counts().filter(pl.col(\"count\") < num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
