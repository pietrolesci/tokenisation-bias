{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import srsly\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_type = \"bpe\"\n",
    "vocab_size = 32_000\n",
    "tok_path = Path(f\"./outputs/tokenizers/{tok_type}{vocab_size}\")\n",
    "out_path = Path(f\"data/slim-pajama-eval-{tok_type}{vocab_size}\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized corpus\n",
    "file_path = out_path / \"data.parquet\"\n",
    "if file_path.exists():\n",
    "    df = pl.read_parquet(file_path)\n",
    "else:\n",
    "    df = (\n",
    "        pl.scan_parquet(f\"hf://datasets/pietrolesci/slim-pajama-eval/{tok_type}{vocab_size}/train-*.parquet\")\n",
    "        .with_columns(tok_pos=pl.int_ranges(pl.col(\"input_ids\").list.len()), seq_len=pl.col(\"input_ids\").list.len())\n",
    "        .collect()\n",
    "    )\n",
    "    df.write_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merges\n",
    "with (tok_path / \"raw_tok_path.txt\").open(\"r\") as fl:\n",
    "    raw_tok_path = Path(fl.read())\n",
    "\n",
    "merges_df = pl.DataFrame(srsly.read_jsonl(raw_tok_path / \"implemented_merges.jsonl\")).with_columns(\n",
    "    pl.col(\"new_token_id\").cast(pl.Int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get window around cutoff\n",
    "num_tok_window = 1_500\n",
    "window_df = merges_df.filter(\n",
    "    (pl.col(\"new_token_id\") >= vocab_size - num_tok_window) & (pl.col(\"new_token_id\") < vocab_size + num_tok_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for in- and out- of vocab tokens around the cutoff\n",
    "in_vocab = window_df.filter(pl.col(\"new_token_id\") < vocab_size)\n",
    "\n",
    "out_vocab = window_df.filter(pl.col(\"new_token_id\") >= vocab_size)\n",
    "out_vocab = (\n",
    "    out_vocab.with_columns(pl.col(\"pair\").list.to_struct())\n",
    "    .unnest(\"pair\")\n",
    "    .rename({\"field_0\": \"tok_a\", \"field_1\": \"tok_b\"})\n",
    "    .with_columns(pl.col(\"tok_a\").cast(pl.Int32), pl.col(\"tok_b\").cast(pl.Int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want at least 3 tokens in the context because\n",
    "# we evaluate on 2 tokens and need 1 for context\n",
    "dfe = df.lazy().explode([\"input_ids\", \"tok_pos\"]).filter(pl.col(\"tok_pos\") >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get uid and tok_pos for tokens in window\n",
    "in_vocab_index = in_vocab.lazy().select([\"new_token_id\"]).join(dfe, left_on=\"new_token_id\", right_on=\"input_ids\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index = (\n",
    "    out_vocab.lazy().select([\"tok_a\", \"tok_b\", \"new_token_id\"])\n",
    "    .join(\n",
    "        dfe.with_columns(next_input_id=pl.col(\"input_ids\").shift(-1)),\n",
    "        left_on=[\"tok_a\", \"tok_b\"],\n",
    "        right_on=[\"input_ids\", \"next_input_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .rename({\"tok_pos\": \"tok_pos_a\"})\n",
    "    .with_columns(tok_pos_b=pl.col(\"tok_pos_a\") + 1)\n",
    "    .filter(pl.col(\"tok_pos_b\") < pl.col(\"seq_len\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check\n",
    "# window_df.filter(\n",
    "#     pl.col(\"new_token_id\")\n",
    "#     .is_in(out_vocab_index[\"new_token_id\"].to_list() + in_vocab_index[\"new_token_id\"].to_list())\n",
    "#     .not_()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample num_samples docs for each token\n",
    "num_samples = 100\n",
    "\n",
    "in_vocab_sample = in_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ").collect()\n",
    "\n",
    "out_vocab_sample = out_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context\n",
    "context_length = 2048\n",
    "\n",
    "in_vocab_df = (\n",
    "    in_vocab_sample.join(df.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos\") > context_length).then(pl.col(\"tok_pos\") - context_length).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos\") - pl.col(\"context_start\") + 1\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# check that last token in context is exactly the token we want to predict\n",
    "assert in_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"new_token_id\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df = (\n",
    "    out_vocab_sample.join(df.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos_b\") > context_length).then(pl.col(\"tok_pos_a\") - context_length + 1).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos_a\") - pl.col(\"context_start\") + 2\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# Check that penultimate token in context is exactly the first token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-2) == pl.col(\"tok_a\"))[\"context\"].all()\n",
    "\n",
    "# Check that last token in context is exactly the second token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"tok_b\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df = in_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\"new_token_id\", \"uid\", \"seq_len\", \"tok_pos\", \"context_start\", \"context_len\", \"context\"]\n",
    ")\n",
    "\n",
    "out_vocab_df = out_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\n",
    "        \"new_token_id\",\n",
    "        \"tok_a\",\n",
    "        \"tok_b\",\n",
    "        \"uid\",\n",
    "        \"seq_len\",\n",
    "        \"tok_pos_a\",\n",
    "        \"tok_pos_b\",\n",
    "        \"context_start\",\n",
    "        \"context_len\",\n",
    "        \"context\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df.write_parquet(out_path / \"in_vocab_samples.parquet\")\n",
    "out_vocab_df.write_parquet(out_path / \"out_vocab_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df = pl.read_parquet(out_path / \"in_vocab_samples.parquet\")\n",
    "out_vocab_df = pl.read_parquet(out_path / \"out_vocab_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb647b376fde4952ba356b82d0ba8084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/299208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_context = pl.concat(\n",
    "    [\n",
    "        in_vocab_df.rename({\"context\": \"input_ids\"}).select([\"new_token_id\", \"uid\", \"input_ids\"]),\n",
    "        out_vocab_df.rename({\"context\": \"input_ids\"}).select([\"new_token_id\", \"uid\", \"input_ids\"]),\n",
    "    ]\n",
    ")\n",
    "all_context = all_context.sort(pl.col(\"input_ids\").list.len(), descending=True)\n",
    "ds = Dataset.from_polars(all_context)\n",
    "ds.save_to_disk(out_path / \"eval_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>new_token_id</th><th>count</th></tr><tr><td>i32</td><td>u64</td></tr></thead><tbody><tr><td>31785</td><td>13</td></tr><tr><td>32154</td><td>95</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────────────┬───────┐\n",
       "│ new_token_id ┆ count │\n",
       "│ ---          ┆ ---   │\n",
       "│ i32          ┆ u64   │\n",
       "╞══════════════╪═══════╡\n",
       "│ 31785        ┆ 13    │\n",
       "│ 32154        ┆ 95    │\n",
       "└──────────────┴───────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "all_context[\"new_token_id\"].value_counts().filter(pl.col(\"count\") < num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
