{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import plotnine as pn\n",
    "import polars as pl\n",
    "import srsly\n",
    "\n",
    "from src.utilities import load_tokenizer_with_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_path = Path(\"/home/pl487/rdd/outputs/tokenizer_train/bpe_2024-09-04T12-59-54/\")\n",
    "data_path = Path(\"data/slim-pajama-subset-validation/\")\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pl.scan_parquet(f\"hf://datasets/pietrolesci/slim-pajama-subset-validation/bpe{vocab_size}/train-*.parquet\")\n",
    "    .with_columns(token_position=pl.int_ranges(pl.col(\"input_ids\").list.len()))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = pl.DataFrame(srsly.read_jsonl(tok_path / \"implemented_merges.jsonl\")).with_columns(\n",
    "    pl.col(\"new_token_id\").cast(pl.Int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get window around cutoff\n",
    "num_tok_window = 1_000\n",
    "df = merges.filter(\n",
    "    (pl.col(\"new_token_id\") >= vocab_size - num_tok_window) & (pl.col(\"new_token_id\") < vocab_size + num_tok_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab = df.filter(pl.col(\"new_token_id\") < vocab_size)\n",
    "out_vocab = df.filter(pl.col(\"new_token_id\") >= vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document uid and position of the token in doc for tokens in vocab\n",
    "in_vocab_index = data.explode([\"input_ids\", \"token_position\"]).join(\n",
    "    in_vocab.select([\"new_token_id\"]), left_on=\"input_ids\", right_on=\"new_token_id\", how=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document uid and position of the token in doc for tokens not in vocab\n",
    "out_vocab = (\n",
    "    out_vocab\n",
    "    .with_columns(pl.col(\"pair\").list.to_struct())\n",
    "    .unnest(\"pair\")\n",
    "    .rename({\"field_0\": \"tok_a\", \"field_1\": \"tok_b\"})\n",
    "    .with_columns(pl.col(\"tok_a\").cast(pl.Int32), pl.col(\"tok_b\").cast(pl.Int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index = (\n",
    "    data\n",
    "    .explode([\"input_ids\", \"token_position\"])\n",
    "    .with_columns(next_input_id=pl.col(\"input_ids\").shift(-1))\n",
    "    .join(out_vocab.select([\"tok_a\", \"tok_b\", \"new_token_id\"]), left_on=[\"input_ids\", \"next_input_id\"], right_on=[\"tok_a\", \"tok_b\"], how=\"right\")\n",
    "    # .drop([\"input_ids\", \"next_input_id\"])\n",
    "    .rename({\"token_position\": \"token_position_a\"})\n",
    "    .with_columns(token_position_b=pl.col(\"token_position_a\") + 1)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docs per each token\n",
    "num_samples = 50\n",
    "\n",
    "in_vocab_sample = in_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ")\n",
    "\n",
    "out_vocab_sample = out_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each doc, get the context (with the token appended) of the required size (+1, since the token is appended)\n",
    "context_length = 2048\n",
    "\n",
    "in_vocab_df = (\n",
    "    in_vocab_sample\n",
    "    .join(data.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"token_position\") > context_length)\n",
    "            .then(pl.col(\"token_position\") - context_length)\n",
    "            .otherwise(0)\n",
    "        ),\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), \n",
    "            length=pl.col(\"token_position\") - pl.col(\"context_start\") + 1,\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])     \n",
    ")\n",
    "\n",
    "# check that last token in context is exactly the token we want to predict\n",
    "assert in_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"new_token_id\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df = (\n",
    "    out_vocab_sample\n",
    "    .join(data.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"token_position_a\") > context_length)\n",
    "            .then(pl.col(\"token_position_a\") - context_length)\n",
    "            .otherwise(0)\n",
    "        ),\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), \n",
    "            length=pl.col(\"token_position_a\") - pl.col(\"context_start\") + 2,\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that last token in context is exactly the first token we want to predict\n",
    "# assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-2) == pl.col(\"tok_a\"))[\"context\"].all()\n",
    "\n",
    "# check that last token in context is exactly the second token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"tok_b\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df.filter(pl.col(\"context\").list.get(-1) != pl.col(\"tok_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.with_columns(index=pl.int_range(pl.len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.with_columns(other=pl.col(\"input_ids\").list.slice((, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_vocab_index.group_by(\"new_token_id\").agg(\n",
    "#     q25=pl.col(\"token_position\").quantile(.25),\n",
    "#     median=pl.col(\"token_position\").median(), \n",
    "#     q75=pl.col(\"token_position\").quantile(.75),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = in_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle().over(\"new_token_id\") < 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep unique doc-tok pairs (if a token appears multiple times in the same doc, keep only one)\n",
    "# which one is kept is not deterministic\n",
    "in_vocab_index = in_vocab_index.unique(subset=[\"uid\", \"new_token_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_index.group_by(\"new_token_id\").agg(num_docs_per_token=pl.col(\"uid\").len()).filter(pl.col(\"num_docs_per_token\") < 100).sort(\"num_docs_per_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.group_by(\"new_token_id\").agg(pl.col(\"uid\").n_unique())[\"uid\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_index.group_by(\"new_token_id\").agg(\n",
    "    uid=pl.col(\"uid\"),\n",
    "    uid=pl.col(\"uid\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index.shu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_of(value) -> pl.Expr:\n",
    "    # https://github.com/pola-rs/polars/issues/5503#issuecomment-1315401973\n",
    "    # only execute if the item is contained in the list\n",
    "    return (\n",
    "        pl.when(pl.col(\"input_ids\").list.contains(value))\n",
    "        .then(\n",
    "            # create array of True/False, then cast to 1's and 0's\n",
    "            # arg_max() then finds the first occurrence of 1, i.e. the first occurence of value\n",
    "            pl.col(\"input_ids\").list.eval((pl.element() == value).cast(pl.UInt8).arg_max(), parallel=True).list.first()\n",
    "        )\n",
    "        .otherwise(None)  # return null if not found\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_context_len = 200\n",
    "max_num_seq = 500\n",
    "seq_with_token = (\n",
    "    data.with_columns(loc=loc_of(14))\n",
    "    .drop_nulls(\"loc\")\n",
    "    # .filter(pl.col(\"loc\") >= min_context_len)\n",
    "    # .sort(\"loc\", descending=True)\n",
    "    .head(max_num_seq)\n",
    "    # .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq_with_token.with_columns(pl.col(\"input_ids\").list.slice(pl.col(\"loc\") - min_context_len, min_context_len + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = split_tokens.with_columns(pl.col(\"pair\").cast(pl.List(pl.String)).list.join(\",\"))[\"pair\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    # .head()\n",
    "    .filter(pl.col(\"input_ids\").cast(pl.List(pl.String)).list.join(\",\").str.contains(q))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][\"input_ids\"].to_list()[0].index(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.explode(\"input_ids\").with_columns(j=pl.first().cumcount().over(\"i\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tok.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = srsly.read_json(path / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame([{\"part_a\": m[0], \"part_b\": m[1]} for m in conf[\"model\"][\"merges\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = load_tokenizer_with_vocab_size(path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tok.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tok.backend_tokenizer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pl.DataFrame({\"tokens\": [i for i in conf[\"model\"][\"vocab\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.join(impl_merges, left_on=\"tokens\", right_on=\"new_token\", how=\"anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pn.ggplot(impl_merges.with_row_index(), pn.aes(y=\"index\", x=\"count\")) + pn.geom_line() + pn.scale_x_log10())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
