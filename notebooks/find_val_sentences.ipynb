{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import srsly\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_path = Path(\"/home/pl487/rdd/outputs/tokenizer_train/bpe_2024-09-04T12-59-54/\")\n",
    "data_path = Path(\"data/slim-pajama-subset-validation/\")\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_type = tok_path.name.split(\"_\")[0]\n",
    "assert tok_type in (\"bpe\",)\n",
    "\n",
    "tok_name = f\"{tok_type}{vocab_size}\"\n",
    "out_path = data_path.parent / f\"{data_path.name}-sample-{tok_name}\"\n",
    "out_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pl.scan_parquet(f\"hf://datasets/pietrolesci/slim-pajama-subset-validation/bpe{vocab_size}/train-*.parquet\")\n",
    "    .with_columns(tok_pos=pl.int_ranges(pl.col(\"input_ids\").list.len()), seq_len=pl.col(\"input_ids\").list.len())\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = pl.DataFrame(srsly.read_jsonl(tok_path / \"implemented_merges.jsonl\")).with_columns(\n",
    "    pl.col(\"new_token_id\").cast(pl.Int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get window around cutoff\n",
    "num_tok_window = 1_000\n",
    "df = merges.filter(\n",
    "    (pl.col(\"new_token_id\") >= vocab_size - num_tok_window) & (pl.col(\"new_token_id\") < vocab_size + num_tok_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab = df.filter(pl.col(\"new_token_id\") < vocab_size)\n",
    "out_vocab = df.filter(pl.col(\"new_token_id\") >= vocab_size)\n",
    "\n",
    "out_vocab = (\n",
    "    out_vocab.with_columns(pl.col(\"pair\").list.to_struct())\n",
    "    .unnest(\"pair\")\n",
    "    .rename({\"field_0\": \"tok_a\", \"field_1\": \"tok_b\"})\n",
    "    .with_columns(pl.col(\"tok_a\").cast(pl.Int32), pl.col(\"tok_b\").cast(pl.Int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document uid and position of the token in doc for tokens in vocab\n",
    "in_vocab_index = data.explode([\"input_ids\", \"tok_pos\"]).join(\n",
    "    in_vocab.select([\"new_token_id\"]), left_on=\"input_ids\", right_on=\"new_token_id\", how=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_index = (\n",
    "    data.explode([\"input_ids\", \"tok_pos\"])\n",
    "    .with_columns(next_input_id=pl.col(\"input_ids\").shift(-1))\n",
    "    .join(\n",
    "        out_vocab.select([\"tok_a\", \"tok_b\", \"new_token_id\"]),\n",
    "        left_on=[\"input_ids\", \"next_input_id\"],\n",
    "        right_on=[\"tok_a\", \"tok_b\"],\n",
    "        how=\"right\",\n",
    "    )\n",
    "    .drop([\"input_ids\", \"next_input_id\"])\n",
    "    .rename({\"tok_pos\": \"tok_pos_a\"})\n",
    "    .with_columns(tok_pos_b=pl.col(\"tok_pos_a\") + 1)\n",
    "    .filter(pl.col(\"tok_pos_b\") < pl.col(\"seq_len\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docs per each token\n",
    "num_samples = 50\n",
    "\n",
    "in_vocab_sample = in_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ")\n",
    "\n",
    "out_vocab_sample = out_vocab_index.filter(\n",
    "    # https://stackoverflow.com/a/72636610\n",
    "    pl.int_range(pl.len()).shuffle(seed=42).over(\"new_token_id\") < num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each doc, get the context (with the token appended) of the required size (+1, since the token is appended)\n",
    "context_length = 2048\n",
    "\n",
    "in_vocab_df = (\n",
    "    in_vocab_sample.join(data.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos\") > context_length).then(pl.col(\"tok_pos\") - context_length).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos\") - pl.col(\"context_start\") + 1\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# check that last token in context is exactly the token we want to predict\n",
    "assert in_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"new_token_id\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df = (\n",
    "    out_vocab_sample.join(data.select([\"uid\", \"input_ids\"]), on=\"uid\", how=\"left\")\n",
    "    .with_columns(\n",
    "        context_start=(\n",
    "            pl.when(pl.col(\"tok_pos_b\") > context_length).then(pl.col(\"tok_pos_a\") - context_length).otherwise(0)\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        context=pl.col(\"input_ids\").list.slice(\n",
    "            offset=pl.col(\"context_start\"), length=pl.col(\"tok_pos_a\") - pl.col(\"context_start\") + 2\n",
    "        )\n",
    "    )\n",
    "    .drop([\"input_ids\"])\n",
    ")\n",
    "\n",
    "# check that last token in context is exactly the first token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-2) == pl.col(\"tok_a\"))[\"context\"].all()\n",
    "\n",
    "# check that last token in context is exactly the second token we want to predict\n",
    "assert out_vocab_df.with_columns(pl.col(\"context\").list.get(-1) == pl.col(\"tok_b\"))[\"context\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df = in_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\"new_token_id\", \"uid\", \"seq_len\", \"tok_pos\", \"context_start\", \"context_len\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_df = out_vocab_df.with_columns(context_len=pl.col(\"context\").list.len()).select(\n",
    "    [\n",
    "        \"new_token_id\",\n",
    "        \"tok_a\",\n",
    "        \"tok_b\",\n",
    "        \"uid\",\n",
    "        \"seq_len\",\n",
    "        \"tok_pos_a\",\n",
    "        \"tok_pos_b\",\n",
    "        \"context_start\",\n",
    "        \"context_len\",\n",
    "        \"context\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab_df.write_parquet(out_path / \"in_vocab.parquet\")\n",
    "out_vocab_df.write_parquet(out_path / \"out_vocab.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context = pl.concat(\n",
    "    [\n",
    "        in_vocab_df.select([\"new_token_id\", \"uid\", \"context\"]), \n",
    "        out_vocab_df.select([\"new_token_id\", \"uid\", \"context\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds = Dataset.from_polars(all_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d56e8cd5f34958a49353940a39395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/99699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(out_path / \"contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
