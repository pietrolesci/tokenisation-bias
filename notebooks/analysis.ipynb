{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datatrove.utils.dataset import DatatroveFolderDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec.core import url_to_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, file_path = url_to_fs(\"hf://datasets/pietrolesci/fineweb-edu-10BT/bpe32000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/pietrolesci/fineweb-edu-10BT/bpe32000/000_bpe32000.ds',\n",
       " 'datasets/pietrolesci/fineweb-edu-10BT/bpe32000/001_bpe32000.ds']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.glob(\"hf://datasets/pietrolesci/fineweb-edu-10BT/bpe32000/*.ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenized data\n",
    "ds = DatatroveFolderDataset(\n",
    "    folder_path=\"hf://datasets/pietrolesci/fineweb-edu-10BT/bpe32000\",\n",
    "    filename_pattern=\"hf://datasets/pietrolesci/fineweb-edu-10BT/bpe32000/*.ds\",\n",
    "    seq_len=512,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    token_size=2 if 32000 < 65_000 else 4,\n",
    ")\n",
    "\n",
    "# Get original data\n",
    "df = pl.scan_parquet(\"hf://datasets/HuggingFaceFW/fineweb-edu/sample/10BT/*.parquet\").tail().collect()\n",
    "\n",
    "# Load tokenizer\n",
    "tok = Tokenizer.from_file(\"outputs/tokenizers/bpe32000/tokenizer.json\")\n",
    "tok.eos_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/pietrolesci/fineweb-edu-10BT/bpe32000/000_bpe32000.ds\n",
      "datasets/pietrolesci/fineweb-edu-10BT/bpe32000/001_bpe32000.ds\n"
     ]
    }
   ],
   "source": [
    "for f in ds.files:\n",
    "    print(f.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19507780"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check batches with ids >= vocab_size\n",
    "vocab_size = tok.get_vocab_size()\n",
    "\n",
    "wrong_batch = []\n",
    "for _i, b in enumerate(tqdm(ds)):\n",
    "    if b[\"input_ids\"].max() >= vocab_size:\n",
    "        wrong_batch.append(b)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31979)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_INDEX = 19493178  #  from here the problem starts\n",
    "ds[START_INDEX][\"input_ids\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds[START_INDEX][\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mod<|endoftext|><|endoftext|><|endoftext|> cases<|endoftext|><|endoftext|><|endoftext|> stat<|endoftext|><|endoftext|><|endoftext|>iology<|endoftext|><|endoftext|><|endoftext|> neur<|endoftext|><|endoftext|><|endoftext|> Finally<|endoftext|><|endoftext|><|endoftext|>John<|endoftext|><|endoftext|><|endoftext|> stood<|endoftext|><|endoftext|><|endoftext|> contributing<|endoftext|><|endoftext|><|endoftext|> diets<|endoftext|><|endoftext|><|endoftext|> Guard<|endoftext|><|endoftext|><|endoftext|> rend<|endoftext|><|endoftext|><|endoftext|> parliament<|endoftext|><|endoftext|><|endoftext|> Educational<|endoftext|><|endoftext|><|endoftext|> billions<|endoftext|><|endoftext|><|endoftext|> priorit<|endoftext|><|endoftext|><|endoftext|>tem<|endoftext|><|endoftext|><|endoftext|> Wang<|endoftext|><|endoftext|><|endoftext|> pear<|endoftext|><|endoftext|><|endoftext|> twin<|endoftext|><|endoftext|><|endoftext|> apnea<|endoftext|><|endoftext|><|endoftext|> risen<|endoftext|><|endoftext|><|endoftext|> antiv<|endoftext|><|endoftext|><|endoftext|> Alexandria<|endoftext|><|endoftext|><|endoftext|> Montgomery<|endoftext|><|endoftext|><|endoftext|>YS<|endoftext|><|endoftext|><|endoftext|> ml<|endoftext|><|endoftext|><|endoftext|> initiation<|endoftext|><|endoftext|><|endoftext|>Give<|endoftext|><|endoftext|><|endoftext|> Lenin<|endoftext|><|endoftext|><|endoftext|>emp<|endoftext|><|endoftext|><|endoftext|> shortness<|endoftext|><|endoftext|><|endoftext|> braces<|endoftext|><|endoftext|><|endoftext|>city<|endoftext|><|endoftext|><|endoftext|>Finding<|endoftext|><|endoftext|><|endoftext|> Kw<|endoftext|><|endoftext|><|endoftext|> powdered<|endoftext|><|endoftext|><|endoftext|> millimeters<|endoftext|><|endoftext|><|endoftext|> recombination<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> includes!<|endoftext|><|endoftext|> town!<|endoftext|><|endoftext|> fert!<|endoftext|><|endoftext|> helpful!<|endoftext|><|endoftext|> Sk!<|endoftext|><|endoftext|> turns!<|endoftext|><|endoftext|>isk!<|endoftext|><|endoftext|>ventions!<|endoftext|><|endoftext|>Get!<|endoftext|><|endoftext|> aging!<|endoftext|><|endoftext|> maxim!<|endoftext|><|endoftext|> struggling!<|endoftext|><|endoftext|>reement!<|endoftext|><|endoftext|> explos!<|endoftext|><|endoftext|> Mathematics!<|endoftext|><|endoftext|>interest!<|endoftext|><|endoftext|>dose!<|endoftext|><|endoftext|> brushing!<|endoftext|><|endoftext|> artillery!<|endoftext|><|endoftext|> 89!<|endoftext|><|endoftext|> studio!<|endoftext|><|endoftext|> Village!<|endoftext|><|endoftext|> 1934!<|endoftext|><|endoftext|> wished!<|endoftext|><|endoftext|> Admiral!<|endoftext|><|endoftext|> Humans!<|endoftext|><|endoftext|> persistence!<|endoftext|><|endoftext|> blessings!<|endoftext|><|endoftext|>toxic!<|endoftext|><|endoftext|>utrient!<|endoftext|><|endoftext|> dy!<|endoftext|><|endoftext|>hero!<|endoftext|><|endoftext|> Drink!<|endoftext|><|endoftext|> insecticides!<|endoftext|><|endoftext|> incubation!<|endoftext|><|endoftext|> Flint!<|endoftext|><|endoftext|> refrigeration!<|endoftext|><|endoftext|> AW!<|endoftext|><|endoftext|> doubles!<|endoftext|><|endoftext|> alfalfa!<|endoftext|><|endoftext|>!<|endoftext|><|endoftext|>!<|endoftext|><|endoftext|>!<|endoftext|><|endoftext|>!<|endoftext|><|endoftext|>!<|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(batch.tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok.decode(ds[len(ds) - 1][\"input_ids\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[-1][\"text\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ds[len(ds) - 1][\"input_ids\"].tolist()\n",
    "s = [tok.decode([i], skip_special_tokens=False) for i in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.encode(tok.decode([t[1]])).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1], s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[0][\"text\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = wrong_batch[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.DataFrame(zip([tok.id_to_token(i) for i in s.tolist()], s.tolist(), strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.token_to_id(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "s = ds[512 * batch_size * 58][\"input_ids\"]\n",
    "s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "\n",
    "conf: dict = srsly.read_json(\"outputs/tokenizers/bpe32000/tokenizer.json\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_in_merge = [tok.token_to_id(i) for j in conf[\"model\"][\"merges\"] for i in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(toks_in_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import plotnine as pn\n",
    "import polars as pl\n",
    "import srsly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.utilities import load_tokenizer_with_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/pl487/rdd/outputs/tokenizer_train/2024-08-30T12-00-43/\")\n",
    "\n",
    "merges = pl.DataFrame(srsly.read_jsonl(path / \"implemented_merges.jsonl\")).with_columns(\n",
    "    pl.col(\"new_token_id\").cast(pl.Int32)\n",
    ")\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(path / \"tok-vocab32000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1000\n",
    "df = merges.filter(\n",
    "    (pl.col(\"new_token_id\") >= tok.vocab_size - window) & (pl.col(\"new_token_id\") < tok.vocab_size + window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens = df.filter(pl.col(\"new_token_id\") < tok.vocab_size)\n",
    "split_tokens = df.filter(pl.col(\"new_token_id\") >= tok.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pl.scan_parquet(\"hf://datasets/pietrolesci/slim-pajama-subset-validation/tok-vocab32000/train-*.parquet\")\n",
    "    .with_columns(token_index=pl.int_ranges(pl.col(\"input_ids\").list.len()))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document uid and position of the token in doc for tokens in vocab\n",
    "token_doc_index = data.explode([\"input_ids\", \"token_index\"]).join(\n",
    "    full_tokens.select([\"new_token_id\"]), left_on=\"input_ids\", right_on=\"new_token_id\", how=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_of(value) -> pl.Expr:\n",
    "    # https://github.com/pola-rs/polars/issues/5503#issuecomment-1315401973\n",
    "    # only execute if the item is contained in the list\n",
    "    return (\n",
    "        pl.when(pl.col(\"input_ids\").list.contains(value))\n",
    "        .then(\n",
    "            # create array of True/False, then cast to 1's and 0's\n",
    "            # arg_max() then finds the first occurrence of 1, i.e. the first occurence of value\n",
    "            pl.col(\"input_ids\").list.eval((pl.element() == value).cast(pl.UInt8).arg_max(), parallel=True).list.first()\n",
    "        )\n",
    "        .otherwise(None)  # return null if not found\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_context_len = 200\n",
    "max_num_seq = 1_000\n",
    "seq_with_token = (\n",
    "    data.with_columns(loc=loc_of(14))\n",
    "    .drop_nulls(\"loc\")\n",
    "    .filter(pl.col(\"loc\") >= min_context_len)\n",
    "    .sort(\"loc\", descending=True)\n",
    "    .head(max_num_seq)\n",
    "    # .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq_with_token.with_columns(pl.col(\"input_ids\").list.slice(pl.col(\"loc\") - min_context_len, min_context_len + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = split_tokens.with_columns(pl.col(\"pair\").cast(pl.List(pl.String)).list.join(\",\"))[\"pair\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    # .head()\n",
    "    .filter(pl.col(\"input_ids\").cast(pl.List(pl.String)).list.join(\",\").str.contains(q))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][\"input_ids\"].to_list()[0].index(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.explode(\"input_ids\").with_columns(j=pl.first().cumcount().over(\"i\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tok.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = srsly.read_json(path / \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame([{\"part_a\": m[0], \"part_b\": m[1]} for m in conf[\"model\"][\"merges\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = load_tokenizer_with_vocab_size(path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tok.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tok.backend_tokenizer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pl.DataFrame({\"tokens\": [i for i in conf[\"model\"][\"vocab\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.join(impl_merges, left_on=\"tokens\", right_on=\"new_token\", how=\"anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pn.ggplot(impl_merges.with_row_index(), pn.aes(y=\"index\", x=\"count\")) + pn.geom_line() + pn.scale_x_log10())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
