# =========================
# Experiment configurations
# =========================
defaults:
  - loggers:
    - tensorboard
  - /callbacks:
    - lr_monitor
    - grad_norm
    - speed_monitor
    - grad_accum
    - model_checkpoint
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  - _self_


hydra:
  # adds colorlog to logging
  job_logging:
    formatters:
      colorlog:
        format: '[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s'
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: bold_red
  # tells hydra to make the run/dir the current working director
  job:
    chdir: true
  # folder structure
  run:
    dir: ./outputs/model_train_pl/${run_folder}
  sweep:
    dir: ./outputs/multirun/model_train_pl
    subdir: ${run_folder}_${hydra.job.id}


run_folder: ${model}_${tok_name}_${now:%Y-%m-%d}T${now:%H-%M-%S}
tok_name: bpe32000minipile
tok_path: /home/pl487/rdd/outputs/tokenizers/${tok_name} # convenience

train_data_path: /home/pl487/rdd/data/minipile/${tok_name}
val_data_path: /home/pl487/rdd/data/minipile-eval-${tok_name}/eval_samples

model: gpt2
resume_from_checkpoint: null
save_initial_checkpoint: true

seed: 42
torch_compile: true


data:
  batch_size: 32
  eval_batch_size: 64
  shuffle: false
  drop_last: false
  num_workers: 8
  pin_memory: true
  persistent_workers: false
  multiprocessing_context: null


optim:
  optim_name: adamw
  lr: 6e-4
  weight_decay: 0.1
  optim_kwargs:
    fused: true
    eps: 1e-8
    betas: [0.9, 0.95]
  scheduler_name: cosine_with_min_lr
  num_warmup_steps: 2000
  scheduler_kwargs:
    min_lr_rate: 0.1
  #   num_stable_steps: 46000
  #   num_decay_steps: 2000
  #   min_lr_ratio: 0


trainer:
  accelerator: gpu
  precision: bf16-true
  deterministic: false
  log_every_n_steps: 1
  enable_progress_bar: true
  fast_dev_run: false
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  val_check_interval: 2000
  max_steps: 100_000
  limit_val_batches: 500
  # limit_train_batches: 10
